# TreeMemoryPredictor üß†

A lightweight, adaptive, and explainable sequence prediction engine based on **Context Mixing** and **Variable Order Markov Models (VOMM)**.

![Python](https://img.shields.io/badge/Python-3.7%2B-blue)
![License](https://img.shields.io/badge/License-MIT-green)
![Dependencies](https://img.shields.io/badge/Dependencies-Zero-lightgrey)

**TreeMemoryPredictor** is designed to predict the next token in a sequence by learning patterns "on the fly." Unlike Neural Networks, it requires **no training epochs**, has **zero cold-start latency**, and adapts instantly to changing data distributions.

It is particularly effective for:
*   Stream processing (IoT sensors, user behavior).
*   Game AI (opponent modeling).
*   Compression algorithms (PPM-style logic).
*   RNG quality testing.

---

## ‚ú® Key Features

1.  **Dynamic Suffix Trie:** Builds a memory structure of variable-length contexts (from $N=1$ to $N_{max}$) in real-time.
2.  **Entropy Scaling ($S^L$):** Automatically adjusts pattern weights based on vocabulary size.
    *   *Binary:* A match of length 3 is weighted by $2^3$.
    *   *Text:* A match of length 3 is weighted by $\approx 30^3$.
    *   *Result:* Long, precise matches in complex alphabets overpower noise automatically.
3.  **Lazy Exponential Decay:** Implements a "forgetting" mechanism ($Count \times Decay^{\Delta t}$). Old, unused patterns fade away, allowing the model to adapt to non-stationary data (concept drift).
4.  **Any Data Type:** Works with integers, strings, characters, or any hashable object.

---

## üöÄ Quick Start

### Installation
Simply copy `tmp.py` (or rename it to `memory_predictor.py`) into your project. There are no heavy dependencies.

### Basic Usage (Binary/Categorical)

```python
from tmp import TreeMemoryPredictor

# Initialize: Look back 6 steps, Decay old patterns by 1% per step
model = TreeMemoryPredictor(n_max=6, decay=0.99)

sequence = [0, 1, 0, 1, 0, 1, 0]

# Online Learning
for token in sequence:
    prediction = model.predict() # Returns 0 or 1
    probabilities = model.predict_proba() # e.g., {0: 0.1, 1: 0.9}
    
    print(f"Input: {token} | Predicted: {prediction} | Confidence: {probabilities.get(prediction, 0):.2f}")
    
    # Update model with actual observation
    model.update(token)
```

### Advanced Usage (Text)

When working with text, the `alphabet_autoscale=True` parameter (on by default) ensures the model understands that guessing a word is much harder than guessing a bit.

```python
text_model = TreeMemoryPredictor(n_max=8, decay=0.95, alphabet_autoscale=True)
sample_text = "hello world hello python"

for char in sample_text:
    pred = text_model.predict()
    text_model.update(char)
```

---

## üî¨ How It Works

The algorithm uses a **weighted voting system** across multiple Markov orders.

### 1. Context Mixing
For a sequence `... A B C`, the model looks up:
*   `C` (Order 1)
*   `B C` (Order 2)
*   `A B C` (Order 3)

### 2. The Weighting Formula
Each node in the Trie calculates its "voting power" using this formula:

$$
Weight = N_{obs} \times (Decay)^{\Delta t} \times (S)^{L}
$$

Where:
*   **$N_{obs}$**: Number of times this pattern was observed.
*   **$Decay$**: Forgetting factor (e.g., 0.99).
*   **$\Delta t$**: Time steps since this node was last visited.
*   **$S$**: Current Vocabulary Size (automatically detected).
*   **$L$**: Length of the pattern.

### 3. Prediction
Probabilities from all context lengths are aggregated and normalized. Due to the $S^L$ term, the longest matching context effectively determines the prediction, while shorter contexts act as a fallback.

---

## ‚öîÔ∏è Comparative Analysis

Where does **TreeMemoryPredictor** fit in the Machine Learning landscape?

It fills the gap between simple statistical counters and heavy Neural Networks. It offers the **precision of compression algorithms** with the **adaptability of online learning**.

| Feature | N-Gram / T9 | Neural Networks (LSTM/GPT) | PPM (Compression) | **TreeMemoryPredictor (TMP)** |
| :--- | :--- | :--- | :--- | :--- |
| **Context Window** | Fixed (e.g., last 2 words) | Long / Attention-based | Variable (Unlimited) | **Variable (1..N)** |
| **Training Time** | None | Hours to Months (GPU) | None | **None (Instant)** |
| **Adaptability** | Low (Static Dictionary) | Low (Frozen Weights) | High | **Very High (Lazy Decay)** |
| **Recall** | Frequency-based | Semantic / Fuzzy | Exact Match | **Exact Match + Decay** |
| **Hardware** | Calculator-tier | GPU Cluster | CPU | **CPU** |
| **Explainability** | High | Black Box | High | **High (Traceable paths)** |

### Detailed Breakdown

#### 1. TMP vs. Standard N-Grams (T9, Gboard)
Standard autocomplete uses a fixed context (e.g., Markov Chain of order 2). It only looks at the last 2 words.
*   **The Flaw:** It cannot capture long-term dependencies (e.g., a repeating phrase of 5 words).
*   **TMP Advantage:** TMP looks at *all* context lengths simultaneously. Thanks to **Entropy Scaling ($S^L$)**, a long unique match (length 10) will mathematically overpower a short frequent match (length 1), allowing it to recall specific quotes or code snippets perfectly.

#### 2. TMP vs. Neural Networks (RNN, Transformers)
Neural Nets learn "fuzzy" representations and semantic meanings.
*   **The Flaw:** They require massive datasets, expensive training, and they "hallucinate" (invent facts). They are hard to update in real-time.
*   **TMP Advantage:** TMP is an **Exact Learner**. It creates a perfect index of what it has seen. It doesn't "understand" concepts, but it remembers patterns with 100% fidelity. It learns instantly‚Äîif it sees a pattern once, it can use it immediately.

#### 3. TMP vs. PPM (Prediction by Partial Matching)
PPM is the gold standard algorithm for text compression (used in RAR/7z). TMP is architecturally inspired by PPM.
*   **The Difference:** Standard PPM uses complex "escape probabilities" to handle unseen data and is designed for static files.
*   **TMP Advantage:** TMP is designed for **Streams**. It introduces the **Exponential Decay** mechanism, which PPM lacks. This allows TMP to "forget" obsolete data and drift with the changing distribution of a live data stream (e.g., user behavior changes).

---

## üìä Performance & Visualization

The repository includes a **Jupyter Notebook** (`experiment.ipynb`) demonstrating:

*   **Sine Wave Reconstruction:** Learning periodic integer sequences.
*   **Pattern Switching:** Adapting to rule changes in real-time.
*   **Natural Language:** Predicting text character-by-character.

*Example of Text Prediction Output:*
> <span style="color:green">Arti</span><span style="color:red">f</span><span style="color:green">icial Intel</span><span style="color:red">l</span><span style="color:green">igen</span><span style="color:red">ce</span>...

---

## üõ† Configuration

All parameters in the `TreeMemoryPredictor` constructor are **optional**. The defaults work well for general-purpose tasks, but you can tune them to fit your specific data dynamics.

| Parameter | Type | Default | Optimal Range | Description |
| :--- | :--- | :--- | :--- | :--- |
| `n_max` | `int` | `10` | `3` - `20` | Maximum context length (Markov order). Higher values capture longer dependencies but increase memory usage. |
| `decay` | `float` | `0.99` | `0.90` - `0.999` | Memory retention rate. `0.90` adapts very fast to changes; `0.999` keeps long-term memory stable. |
| `alphabet_autoscale` | `bool` | `True` | `True` | **Highly Recommended.** Scales weights by $VocabSize^{Length}$. Set to `False` only for strict binary tasks if you prefer raw $2^L$ weighting. |

---

## üìö Theoretical Foundations

This project sits at the intersection of several classic algorithms. While implemented from scratch for stream processing, it shares DNA with:

*   **Prediction by Partial Matching (PPM):** A statistical data compression technique. Like PPM, this model uses a Suffix Trie to find the longest matching context. However, instead of using "escape probabilities" (standard PPM), we use a weighted **Context Mixing** strategy.
*   **Variable Order Markov Models (VOMM):** Unlike a standard Markov Chain that looks back exactly $N$ steps, this model looks back $1, 2, ..., N$ steps simultaneously and aggregates the results based on their confidence.
*   **Exponential Moving Average (EMA):** The "lazy decay" mechanism is effectively an EMA applied to token counts, allowing the system to handle *concept drift* (when the data source changes its behavior over time).

---

## ü§ù Contributing

Contributions are welcome! Feel free to open an issue or submit a Pull Request if you have ideas for optimizing the Trie structure or adding new weighting schemes.

## üìÑ License

This project is licensed under the MIT License.